<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>基于友链的中文独立博客发现</title><meta name="description" content="Perverted degenerate"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.png"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="怎么大家都有朋友.jpg


缘起2022 年某天晚上我突发奇想：
已知很多博客都有友链，它们之间常常呈交叉链接关系。那么，如果我能编写一只自动追踪友链的爬虫，沿着友链不断递归前进，最后能否穷尽所有（有友链的）中文独立博客？
这个需求从爬虫的角度来看其实十分简单。待爬取链接全都是首页，无需解析复杂的数据结构，大部分目标为静态网站，并且几乎没有反爬措施。真正的难点在于非工程的部分：怎样判断一个链接是不是博客，且如何获取它的友链？
思路这个程序的思路十分原始且暴力。
从 x 个带有友链的博客开始：

读取首页和 Sitemap，寻找可能为友链的页面链接。
读取友链页面，提取出页面中所有的外站链接。
简单地认为这些外站链接都是潜在的博客，重复 1。

1 的判断标准其实还挺简单，最后定格为如下条件：
# 检查UR.."><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Chatnoir Miki's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">基于友链的中文独立博客发现</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%98%E8%B5%B7"><span class="toc-text">缘起</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E8%B7%AF"><span class="toc-text">思路</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E5%A6%82%E6%97%A2%E5%BE%80%E5%9C%B0%E7%BC%96%E5%86%99Shit"><span class="toc-text">一如既往地编写 Shit</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%8D%89%E9%B8%A1%E6%B6%85%E6%A7%83"><span class="toc-text">草鸡涅槃</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E8%BF%87%E7%A8%8B%E4%B8%8E%E6%95%B0%E6%8D%AE%E8%BF%87%E6%BB%A4"><span class="toc-text">爬取过程与数据过滤</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AC%E8%87%AA%E5%8A%A8%E6%9C%BA"><span class="toc-text">AC 自动机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A6%81%E6%AD%A2%E8%B6%85%E9%80%9F%E8%A1%8C%E9%A9%B6"><span class="toc-text">禁止超速行驶</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%B6%E6%95%9B"><span class="toc-text">收敛</span></a></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/%E5%B0%8F%E9%A1%B9%E7%9B%AE"><i class="tag post-item-tag">小项目</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">基于友链的中文独立博客发现</h1><time class="has-text-grey" datetime="2022-07-16T11:52:30.000Z">2022-07-16</time><article class="mt-2 post-content"><p>怎么大家都有朋友.jpg</p>
<span id="more"></span>

<h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>2022 年某天晚上我突发奇想：</p>
<p>已知很多博客都有友链，它们之间常常呈交叉链接关系。那么，如果我能编写一只自动追踪友链的爬虫，沿着友链不断递归前进，最后能否穷尽所有（有友链的）中文独立博客？</p>
<p>这个需求从爬虫的角度来看其实十分简单。待爬取链接全都是首页，无需解析复杂的数据结构，大部分目标为静态网站，并且几乎没有反爬措施。真正的难点在于非工程的部分：怎样判断一个链接是不是博客，且如何获取它的友链？</p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>这个程序的思路十分原始且暴力。</p>
<p>从 x 个带有友链的博客开始：</p>
<ol>
<li>读取首页和 Sitemap，寻找可能为友链的页面链接。</li>
<li>读取友链页面，提取出页面中所有的外站链接。</li>
<li>简单地认为这些外站链接都是潜在的博客，重复 1。</li>
</ol>
<p>1 的判断标准其实还挺简单，最后定格为如下条件：</p>
<pre><code class="python"># 检查URL和链入标题, 是否包含特定关键词
links_keyword = ['friend', 'link']
title_keyword = ['友情链接', '友链', '朋友', '友人', 'Friend']
</code></pre>
<p>2 也没有太大难度，过滤一下 <code>domain</code> 和本家相同的就可以。</p>
<p>小步快跑、快速迭代一向是我的特长，这种级别的需求我一般可以在几小时内给出解决方案，但代价绝对是可维护性为 0… 这次也不例外。</p>
<h2 id="一如既往地编写Shit"><a href="#一如既往地编写Shit" class="headerlink" title="一如既往地编写Shit"></a>一如既往地编写 Shit</h2><p>编写第一版程序的时候低估了工作量，没有使用 ORM 和爬虫框架，采用 sqlite3 和 pyppeteer 爆肝一下午纯天然手工编写而成。为此自己给数据建了模，还写了一大堆数据库测试，头发数量减少（不是很难，但恶心）。</p>
<p>然而在扩展时发现第一版实现的 <code>Url</code> 和 <code>Blog</code> 类型存在重大缺陷，设计上考虑不周导致类型混乱，<code>blog.Url.url_str</code> 这种类型不明的屎满天飞。此外，数据库保存的是 URL 而非域名，因此同一个域的不同 path 会被多次计入，<code>http</code> 和 <code>https</code> 也会被当作两个条目分别保存。项目结构很水，无力修改，再加上天生单线程，于是最终作废，光荣进入垃圾堆。</p>
<p>你可以在<a target="_blank" rel="noopener" href="https://git.amono.me/AsterisMono/Influnc">这里</a>找到第一版程序。</p>
<h2 id="草鸡涅槃"><a href="#草鸡涅槃" class="headerlink" title="草鸡涅槃"></a>草鸡涅槃</h2><p>第二版开始设计的时候吸取了第一版的教训，在保持数据库结构简洁的同时，选择保存博客域名而非地址（方便查重，但是对不起那些形如 <code>www.myspace.com/blog</code> 的同学）。从第一版数据的分析结果中，还总结出了一个博客条目的几种状态：在线（友链）、在线（无友链）、离线。</p>
<p>这次学乖了，人生苦短，我用框架。ORM 选用了 <a target="_blank" rel="noopener" href="https://github.com/coleifer/peewee">peewee</a>，爬虫框架当然是大名鼎鼎的 <a target="_blank" rel="noopener" href="https://github.com/scrapy/scrapy">scrapy</a>，边玩边学，寓教于乐。</p>
<p>最终代码质量我觉得（相对我以前写的屎来说）还算可以，开箱即用，内置几个过滤组件和数据库；你可以在<a target="_blank" rel="noopener" href="https://github.com/AsterisMono/InfluncPlus">这里（GIthub）</a>或<a target="_blank" rel="noopener" href="https://git.amono.me/AsterisMono/InfluncPlus">这里（Gitea）</a>找到第二版代码，欢迎 Star。</p>
<h2 id="爬取过程与数据过滤"><a href="#爬取过程与数据过滤" class="headerlink" title="爬取过程与数据过滤"></a>爬取过程与数据过滤</h2><p>世事无常，大肠包小肠。</p>
<p>在不断迭代爬取数据的过程中，我收获了包括且不限于如下惊喜：</p>
<p>xx 人才网 where xx in 中国城市列表；化学材料网、螺管螺帽网；麦片站群和他的爹地赌狗站；软件园、下载站；各种在线离线无米线站长工具、签到工具、下载工具、百度云解析、阿里云列表、网易云音乐解锁；各种官网（云服务、Vultr、Hexo、Cloudflare，甚至学习强国）；各种商业网站（SEO、图库、垃圾站群、养生、证券）；私人云盘、私人图床还有小姐姐。</p>
<p>而且更残酷的事实是，这些站普遍都很重道义地挂了友情链接，也就是说 引一匹狼入数据库 = 狼均引流狗 = 大面积数据污染。讲真我非常好奇这些链接最终追溯回去是哪位老哥引流进来的（虽然最后也没查）…</p>
<p>因为我有一边爬一边盯着数据库看的习惯，所以发现还算及时。第一批被发现的是 xx 人才网，应对方案也很简单，写一个屏蔽词列表，加入一个 <code>FilterPipeline</code> 把 <code>title</code> 看起来不对的条目丢弃就可以。</p>
<p>首先荣登黑名单的是全国各大城市名称和省份名称（对不起把自己的定位写在标题里的同学们，你们被无情抛弃惹）。然后是一些常见单词，和一些常见但不能写在这里的单词。在不断爬取 - 检查 - 丰富名单 - 重新开始的过程中，屏蔽词不断发展壮大，最终条目数量居然达到了 500 左右。这个数目如果还写成 <code>list</code>，每个条目都会遍历一次 <code>list</code>，然后每个 <code>list</code> 中的条目都要做一次字符串匹配，感觉代价有一点不可接受，遂咕果，寻找更好的方案。</p>
<h2 id="AC自动机"><a href="#AC自动机" class="headerlink" title="AC自动机"></a>AC 自动机</h2><p>咕果指路：<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm">ahocorasick</a>。</p>
<p>简单了解了一下，AC 自动机能在字符串中快速匹配字典中的字串… 那就决定是它了。</p>
<p>Stackoverflow 拼凑之，弄出一只单例 <code>Tester()</code>，在爬虫内外大肆使用。为了调试方便还实现了一个屏蔽词热更新功能：当爬取过程中检测到屏蔽词库发生变化时，会自动重载词库并重新生成自动机，造福人类。</p>
<h2 id="禁止超速行驶"><a href="#禁止超速行驶" class="headerlink" title="禁止超速行驶"></a>禁止超速行驶</h2><p>爬取过程中遇到两个非常搞笑的 BUG。</p>
<p>第一，数据库最开始有两个条目，理论上会从这两个条目派生出至少 50 条，然后系统就可以全速并行爬取。但是这玩意和冬天的老爷车犯同一个毛病：第一次打火，不一会就熄火，得多打两次才能启动。</p>
<p>一顿头脑风暴后发现，提供待爬取条目的函数长这样：</p>
<pre><code class="python">def start_requests(self):
    cleanup_database()
    while has_unaccessed_blog():
        blog = get_unaccessed_blog()
        blog.status = "pending"
        blog.save()
        url = "https://" + blog.domain
        yield scrapy.Request(url, callback=self.parse_blog, errback=self.error_handling, cb_kwargs={'src': blog})
</code></pre>
<p>注意那个 <code>while</code>。在初始条目较少的情况下，它们会很快地被 <code>yield</code> 出去，然后这个函数就会因为 <code>has_unaccessed_blog == False</code> 而退出，后续添加进来的条目也就没有办法被看到。</p>
<p>解决方案是？当然是不解决。预热一次左右就可以全速爬取，无非是多点一下鼠标的事情。</p>
<p>第二，有些人的博客首页不太规范。这个程序不知什么原因，遇到首页 SSL 证书货不对板、或者无限重定向的情况，那个爬取线程就会不声不响的退出，在数据库里留下来一个 <code>pending</code> 的烂摊子，有时候还会导致整个爬取进程都卡住不动。这个我也没有找到很好的解决方案（DEBUG 日志都不会输出，怀疑是内部 Bug），只能在最后爬虫收敛的时候手动在数据库里进行标记。</p>
<h2 id="收敛"><a href="#收敛" class="headerlink" title="收敛"></a>收敛</h2><p>这个其实在我的意料之中，博客条目们最终一定会收敛成一个类似离散数学中群的东西，任何一个条目的所有友链都存在于这个数据库，也就不再会有新的条目。</p>
<p>经过机器过滤和人工过滤后，总条目数定格在 12474 条。其中在线 7116 条（含友链 3920 条，不含友链 3196 条），离线 4246 条，其余被标记删除。</p>
<p>粗略翻看了一下，个人认为准确率相当不错，在翻看途中也发现了很多自己平时就经常拜访的博客，感觉十分奇妙。这份数据除了当作订阅列表外，因其保存了博客之间的友链关系，还具有潜在的科研价值。</p>
<p>由于潜在的侵权和法律相关问题，我最终决定还是不公开得到的数据库。但由于短时间内大家的友链不太可能发生改变，想要同款数据的同学可以下载源码自行爬取，开袋即食。</p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2022/07/31/Linux%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" title="Linux 中的常用命令"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: Linux 中的常用命令</span></a><a class="button is-default" href="/2022/07/16/Rust%E4%B8%AD%E7%9A%84%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/" title="Rust 中的智能指针"><span class="has-text-weight-semibold">Next: Rust 中的智能指针</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/AsterisMono"><i class="iconfont icon-github"></i></a><!-- Ins--><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Chatnoir Miki 2022</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/post.js"></script></body></html>